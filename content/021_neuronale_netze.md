## Künstliche neuronale Netze {#kNN}

### Ursprung

Künstliche neuronale Netze, kurz kNN, sind der Versuch, die aus der Neurowissenschaft bekannten Konzepte der neuronalen Netze, künstlich nachzubauen. Dabei erhofft man sich an die Leistung des Gehirns künstlich anschließen zu können.

KNN bestehen aus Neuronen, welche untereinander zu einem Netz verbunden werden. Dabei wird zwischen den Eingangsneuronen, unsichtbaren Neuronen und Ausgangsneuronen unterschieden. [vgl. @ki-norvig, S.845]

### Aufbau {#kNN_aufbau}

Ein kNN besitzt immer je eine Schicht von Eingangs- und Ausgangsneuronen. Dazwischen können sich keine, eine oder mehrere Schichten von unsichtbaren Neuronen befinden. Diese mittlere Schichten werden "unsichtbar" genannt, da auf diese von außen in der Regel nicht zugegriffen werden kann.

Besitzt ein kNN keine unsichtbare Schicht handelt es sich um ein Kernel-Perzeptron. Auf diese wird im Rahmen dieser Arbeit nicht eingegangen.

Besitzt ein kNN eine unsichtbare Schicht handelt es sich um ein einschichtiges kNN oder auch Perzeptorn-Netz genannt. Perzeptron-Netze sind universal, das heißt sie sind theoretisch im Stande jede beliebige Funktion darzustellen.

Besitzt ein kNN mehrere unsichtbare Schichten, spricht man auch von einem mehrschichtigen kNN. Im Englischen auch MLP, Multi Layered Perceptron, genannt. Bei der Verwendung von mehrschichtigen kNN wird auch häufig von Deep-Learning gesprochen. Deep, da diese eine gewisse tiefe durch die unsichtbaren Schichten besitzen. [vgl. @ki-norvig, S.846-850]

### Neuron

Die Neuronen bilden das Herz eines kNN. Dabei Funktionieren die Neuronen durch eine Eingabefunktion Aktivierungsfunktion und der Ausgabe.

#### Eingabefunktion



#### Aktivierungsfunktion

#### Ausgabe




### Gradienten abstiegsverfahren

### Kostenfunktion

### Backpropagation Algorithmus

### Gute Startwerte finden

